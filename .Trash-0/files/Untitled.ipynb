{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33878d7d-bf58-4f2e-a48b-debbe07ef6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType, DoubleType, TimestampType\n",
    "\n",
    "import utils.data_processing_bronze_table\n",
    "import utils.data_processing_silver_table\n",
    "import utils.data_processing_gold_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a536aa-c6e3-4889-82dc-c6e88daf65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 11:46:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103eae-27ef-4dbb-a862-766b3abfa7b0",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c68a5b4-0b06-4fe8-bb18-6d966d44bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_base    = \"data/raw\"               # raw\n",
    "bronze_base = \"datamart/bronze\"        # Bronze \n",
    "\n",
    "tables = {\n",
    "    \"feature_clickstream\":         \"feature_clickstream.csv\",\n",
    "    \"features_attributes\": \"features_attributes.csv\",\n",
    "    \"features_financials\": \"features_financials.csv\",\n",
    "    \"lms_loan_daily\":     \"lms_loan_daily.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103680c6-e352-4f24-8947-e9f1341e049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bronze] is writed  feature_clickstream to datamart/bronze/feature_clickstream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bronze] is writed  features_attributes to datamart/bronze/features_attributes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bronze] is writed  features_financials to datamart/bronze/features_financials\n",
      "[Bronze] is writed  lms_loan_daily to datamart/bronze/lms_loan_daily\n"
     ]
    }
   ],
   "source": [
    "for tbl_name, filename in tables.items():\n",
    "    # 1) raw path\n",
    "    raw_path = os.path.join(raw_base, filename)\n",
    "    # 2) automatically choose the reading format\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = spark.read \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .csv(raw_path)\n",
    "    elif filename.endswith(\".json\"):\n",
    "        df = spark.read.json(raw_path)\n",
    "    elif filename.endswith(\".parquet\"):\n",
    "        df = spark.read.parquet(raw_path)\n",
    "    else:\n",
    "        raise ValueError(f\"not supported file format：{filename}\")\n",
    "\n",
    "    # 3) add a timestamp to all the tables\n",
    "    df = df.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "\n",
    "    # 4) write to bronze table\n",
    "    out_path = os.path.join(bronze_base, tbl_name)\n",
    "    df.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "    print(f\"[Bronze] is writed  {tbl_name} to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0130f024-5f06-417c-bc93-866ec2f44cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/data_processing_bronze_table.py\n",
    "\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def process_bronze(spark, raw_base, bronze_base, tables):\n",
    "    \"\"\"\n",
    "    - spark: SparkSession\n",
    "    - raw_base: 原始数据文件夹\n",
    "    - bronze_base: Bronze 层根目录\n",
    "    - tables: dict{name: filename}\n",
    "    \"\"\"\n",
    "    for tbl_name, filename in tables.items():\n",
    "        raw_path = os.path.join(raw_base, filename)\n",
    "        # 根据后缀自动读取\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = spark.read.option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .csv(raw_path)\n",
    "        elif filename.endswith(\".json\"):\n",
    "            df = spark.read.json(raw_path)\n",
    "        elif filename.endswith(\".parquet\"):\n",
    "            df = spark.read.parquet(raw_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {filename}\")\n",
    "        # 加入摄取时间\n",
    "        df = df.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "        # 写出\n",
    "        out_path = os.path.join(bronze_base, tbl_name)\n",
    "        df.write.mode(\"overwrite\").parquet(out_path)\n",
    "        print(f\"[Bronze] 写入 {tbl_name} → {out_path}\")\n",
    "    # 若需要，可以返回所有 DF 的 dict\n",
    "    # return {name: spark.read.parquet(os.path.join(bronze_base,name)) for name in tables}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562efa-2705-487c-8c5c-7fa3c9812276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
