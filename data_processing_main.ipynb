{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26246edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d54f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 14:59:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81892b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "raw_base    = \"data/raw\"               # raw\n",
    "bronze_base = \"datamart/bronze\"        # Bronze\n",
    "\n",
    "\n",
    "tables = {\n",
    "    \"feature_clickstream\":         \"feature_clickstream.csv\",\n",
    "    \"features_attributes\": \"features_attributes.csv\",\n",
    "    \"features_financials\": \"features_financials.csv\",\n",
    "    \"lms_loan_daily\":     \"lms_loan_daily.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e4b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bronze] 已写入表 feature_clickstream 到 datamart/bronze/feature_clickstream\n",
      "[Bronze] 已写入表 features_attributes 到 datamart/bronze/features_attributes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bronze] 已写入表 features_financials 到 datamart/bronze/features_financials\n",
      "[Bronze] 已写入表 lms_loan_daily 到 datamart/bronze/lms_loan_daily\n"
     ]
    }
   ],
   "source": [
    "for tbl_name, filename in tables.items():\n",
    "    # 1) find the raw base path\n",
    "    raw_path = os.path.join(raw_base, filename)\n",
    "    # 2) autumatically read the format\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = spark.read \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .csv(raw_path)\n",
    "    elif filename.endswith(\".json\"):\n",
    "        df = spark.read.json(raw_path)\n",
    "    elif filename.endswith(\".parquet\"):\n",
    "        df = spark.read.parquet(raw_path)\n",
    "    else:\n",
    "        raise ValueError(f\"not supported format：{filename}\")\n",
    "\n",
    "    # 3) add a current timestamp for each table\n",
    "    df = df.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "\n",
    "    # 4) write to bronze table\n",
    "    out_path = os.path.join(bronze_base, tbl_name)\n",
    "    df.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "    print(f\"[Bronze] is successfully writting {tbl_name} to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a173f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver & Gold 构建完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, to_date, regexp_replace, when, row_number\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# —— initialize SparkSession ——\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# —— set the cata ——\n",
    "bronze_base = \"datamart/bronze\"\n",
    "silver_base = \"datamart/silver\"\n",
    "gold_base   = \"datamart/gold\"\n",
    "\n",
    "# —— 1. Silver cleaning & transforming ——\n",
    "\n",
    "# 1.1 Clickstream feature\n",
    "click_df = spark.read.parquet(os.path.join(bronze_base, \"feature_clickstream\"))\n",
    "for c in click_df.columns:\n",
    "    if c.startswith(\"fe_\"):\n",
    "        click_df = click_df.withColumn(c, col(c).cast(IntegerType()))\n",
    "click_df = click_df.na.fill({c: 0 for c in click_df.columns if c.startswith(\"fe_\")})\n",
    "click_df.write.mode(\"overwrite\").parquet(os.path.join(silver_base, \"feature_clickstream\"))\n",
    "\n",
    "# 1.2 attribute\n",
    "attr_df = spark.read.parquet(os.path.join(bronze_base, \"features_attributes\"))\n",
    "attr_df = attr_df.withColumn(\"snapshot_date\", to_date(\"snapshot_date\", \"yyyy/M/d\"))\n",
    "attr_df = attr_df.withColumn(\"Age\", regexp_replace(\"Age\", \"[^0-9]\", \"\").cast(IntegerType()))\n",
    "attr_df = attr_df.withColumn(\n",
    "    \"SSN\",\n",
    "    when(col(\"SSN\").rlike(r\"^\\d{3}-\\d{2}-\\d{4}$\"), col(\"SSN\")).otherwise(None)\n",
    ")\n",
    "attr_df.write.mode(\"overwrite\").parquet(os.path.join(silver_base, \"features_attributes\"))\n",
    "\n",
    "# 1.3 financial\n",
    "fin_df = spark.read.parquet(os.path.join(bronze_base, \"features_financials\"))\n",
    "fin_df = fin_df.withColumn(\"snapshot_date\", to_date(\"snapshot_date\", \"yyyy/M/d\"))\n",
    "for c in [\n",
    "    \"Customer_Annual_Inc\",\"Monthly_IR\",\"Num_Bank_Accounts\",\"Num_Credit_Ca\",\n",
    "    \"Interest_Rate\",\"Num_of_Loan\",\"Delay_from_due_date\",\"Num_of_Delayed_Payment\",\n",
    "    \"Changed_Cred\",\"Num_Credit_In\",\"Outstanding_D\",\"Credit_Utilizati\",\n",
    "    \"Total_EMI_per_M\",\"Amount_invest\"\n",
    "]:\n",
    "    if c in fin_df.columns:\n",
    "        fin_df = fin_df.withColumn(c, col(c).cast(FloatType()))\n",
    "fin_df = fin_df.withColumn(\n",
    "    \"Credit_Mix\",\n",
    "    when(col(\"Credit_Mix\").isin(\"\", \"_\"), None).otherwise(col(\"Credit_Mix\"))\n",
    ")\n",
    "fin_df.write.mode(\"overwrite\").parquet(os.path.join(silver_base, \"features_financials\"))\n",
    "\n",
    "# 1.4 loan\n",
    "loan_df = spark.read.parquet(os.path.join(bronze_base, \"lms_loan_daily\"))\n",
    "loan_df = loan_df \\\n",
    "    .withColumn(\"loan_start_date\", to_date(\"loan_start_date\", \"yyyy/M/d\")) \\\n",
    "    .withColumn(\"snapshot_date\",   to_date(\"snapshot_date\",   \"yyyy/M/d\"))\n",
    "for c in [\"loan_amt\",\"due_amt\",\"paid_amt\",\"overdue_amt\",\"balance\"]:\n",
    "    loan_df = loan_df.withColumn(c, col(c).cast(FloatType()))\n",
    "loan_df = loan_df.withColumn(\"dpd_flag\", when(col(\"overdue_amt\") > 0, 1).otherwise(0))\n",
    "loan_df.write.mode(\"overwrite\").parquet(os.path.join(silver_base, \"lms_loan_daily\"))\n",
    "\n",
    "# —— 1.5 drop ingest_time field ——  \n",
    "attr_df  = attr_df.drop(\"ingest_time\")\n",
    "fin_df   = fin_df.drop(\"ingest_time\")\n",
    "click_df = click_df.drop(\"ingest_time\")\n",
    "loan_df  = loan_df.drop(\"ingest_time\")\n",
    "\n",
    "\n",
    "# —— 2. Gold summary & concat ——\n",
    "\n",
    "w_attr = Window.partitionBy(\"Customer_ID\").orderBy(col(\"snapshot_date\").desc())\n",
    "attr_latest = (\n",
    "    attr_df\n",
    "    .withColumn(\"rn\", row_number().over(w_attr))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    "    .withColumnRenamed(\"snapshot_date\", \"attr_snapshot_date\")\n",
    ")\n",
    "\n",
    "w_fin = Window.partitionBy(\"Customer_ID\").orderBy(col(\"snapshot_date\").desc())\n",
    "fin_latest = (\n",
    "    fin_df\n",
    "    .withColumn(\"rn\", row_number().over(w_fin))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    "    .withColumnRenamed(\"snapshot_date\", \"fin_snapshot_date\")\n",
    ")\n",
    "\n",
    "# 2.2 Loan \n",
    "loan_summary = loan_df.groupBy(\"Customer_ID\").agg(\n",
    "    F.count(\"*\").alias(\"loan_records\"),\n",
    "    F.sum(\"dpd_flag\").alias(\"total_dpd_count\"),\n",
    "    F.sum(\"loan_amt\").alias(\"total_loan_amount\"),\n",
    "    F.max(\"snapshot_date\").alias(\"last_loan_snapshot\")\n",
    ")\n",
    "\n",
    "# 2.3 form the user profile\n",
    "gold_df = (\n",
    "    attr_latest\n",
    "      .join(fin_latest,   \"Customer_ID\", \"left\")\n",
    "      .join(click_df,     \"Customer_ID\", \"left\")\n",
    "      .join(loan_summary, \"Customer_ID\", \"left\")\n",
    ")\n",
    "\n",
    "# write Gold level\n",
    "gold_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(os.path.join(gold_base, \"customer_profile\"))\n",
    "\n",
    "print(\"✅ Silver & Gold successfully build！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
